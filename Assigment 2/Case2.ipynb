{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import joblib\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, KFold\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_host = pd.read_csv('host_data.csv')\n",
    "df_loc = pd.read_csv('location_data.csv')\n",
    "df_pric = pd.read_csv('pricing_data.csv')\n",
    "df_prop = pd.read_csv('property_data.csv')\n",
    "df_rev = pd.read_csv('review_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "customer_id                   0\n",
      "host_since                  173\n",
      "host_response_rate        17193\n",
      "host_has_profile_pic        173\n",
      "host_identity_verified      173\n",
      "dtype: int64\n",
      "customer_id         0\n",
      "neighbourhood    9550\n",
      "latitude            0\n",
      "longitude           0\n",
      "city                0\n",
      "zipcode           894\n",
      "dtype: int64\n",
      "customer_id            0\n",
      "log_price              0\n",
      "cleaning_fee           0\n",
      "cancellation_policy    0\n",
      "dtype: int64\n",
      "customer_id        0\n",
      "property_type      0\n",
      "room_type          0\n",
      "bedrooms          83\n",
      "bathrooms        182\n",
      "accommodates       0\n",
      "beds             113\n",
      "dtype: int64\n",
      "customer_id                 0\n",
      "number_of_reviews           0\n",
      "review_scores_rating    15733\n",
      "first_review            14932\n",
      "last_review             14896\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "dfs = [df_host, df_loc, df_pric, df_prop, df_rev]\n",
    "for i in dfs:\n",
    "    print(i.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [df_host, df_loc, df_pric, df_prop, df_rev]\n",
    "for i in dfs:\n",
    "    print(i.shape)\n",
    "    print(i.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, df in enumerate(dfs, start=1):\n",
    "    num_duplicates = df.duplicated(subset=['customer_id'], keep=False).sum()\n",
    "    print(f\"DataFrame {i}: {num_duplicates} duplicate rows based on 'customer_id'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, df in enumerate(dfs, start=1):\n",
    "    num_duplicates = df.duplicated(keep=False).sum()\n",
    "    print(f\"DataFrame {i}: {num_duplicates} fully identical rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_host = df_host.drop_duplicates(keep='first')\n",
    "df_loc = df_loc.drop_duplicates(keep='first')\n",
    "df_pric = df_pric.drop_duplicates(keep='first')\n",
    "df_prop = df_prop.drop_duplicates(keep='first')\n",
    "df_rev = df_rev.drop_duplicates(keep='first')\n",
    "dfs = [df_host, df_loc, df_pric, df_prop, df_rev]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in dfs:\n",
    "    print(i.shape)\n",
    "    print(i.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_host.merge(df_loc, on='customer_id', how='inner') \\\n",
    "                   .merge(df_pric, on='customer_id', how='inner') \\\n",
    "                   .merge(df_rev, on='customer_id', how='inner') \\\n",
    "                   .merge(df_prop, on='customer_id', how='inner')\n",
    "df['host_since'] = pd.to_datetime(df['host_since'])\n",
    "df['first_review'] = pd.to_datetime(df['first_review'])\n",
    "df['last_review'] = pd.to_datetime(df['last_review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review_scores_rating'] = df['review_scores_rating'].fillna(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['host_identity_verified'] = df['host_identity_verified'].fillna(df['host_identity_verified'].mode()[0])\n",
    "df['host_has_profile_pic'] = df['host_has_profile_pic'].fillna(df['host_has_profile_pic'].mode()[0])\n",
    "\n",
    "df['host_response_rate'] = df['host_response_rate'].astype(str).str.rstrip('%')\n",
    "df['host_response_rate'] = pd.to_numeric(df['host_response_rate'], errors='coerce')\n",
    "median_value = df['host_response_rate'].median()\n",
    "df['host_response_rate'].fillna(median_value, inplace=True)\n",
    "\n",
    "df['host_since'] = (pd.to_datetime(datetime.now()) - df['host_since']).dt.days / 365\n",
    "df['host_since'] = df['host_since'].fillna(df['host_since'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['host_response_rate'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['bathrooms'] = df['bathrooms'].fillna(df['bathrooms'].mode()[0])\n",
    "df['beds'] = df['beds'].fillna(df['beds'].mode()[0])\n",
    "df['bedrooms'] = df['bedrooms'].fillna(df['bedrooms'].mode()[0])\n",
    "\n",
    "df['bedrooms'] = df['bedrooms'].replace(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change t/f to True and False\n",
    "df['host_has_profile_pic'] = df['host_has_profile_pic'].map({'t': True, 'f': False}).astype(bool)\n",
    "df['host_identity_verified'] = df['host_identity_verified'].map({'t': True, 'f': False}).astype(bool)\n",
    "#Capatalize values in room_type and property_type to have a uniform standard between values\n",
    "df['room_type'] = df['room_type'].str.upper()\n",
    "df['property_type'] = df['property_type'].str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fill_loc_data import FillLocData\n",
    "if (df['zipcode'].isna().sum() > 0) and (os.path.exists('df_filled.csv') ):\n",
    "    df = pd.read_csv('df_filled.csv' , index_col = None)\n",
    "\n",
    "if (df['zipcode'].isna().sum() > 0) : \n",
    "    df = FillLocData().fill_zipcode(df)\n",
    "    df.to_csv('df_filled.csv' , index=False)\n",
    "\n",
    "if (df['neighbourhood'].isna().sum() > 0) : \n",
    "    df = FillLocData().fill_neighbourhood(df)\n",
    "    df.to_csv('df_filled.csv' , index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols = df.select_dtypes(include=['number']).columns\n",
    "\n",
    "num_cols = len(numerical_cols)\n",
    "fig, axes = plt.subplots(1, num_cols, figsize=(4 * num_cols, 6))\n",
    "for ax, col in zip(axes, numerical_cols):\n",
    "    sns.boxplot(data=df, y=col, ax=ax)\n",
    "    ax.set_title(f'Box Plot of {col}')\n",
    "    ax.set_ylabel(col)\n",
    "    ax.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['customer_id' , 'latitude','longitude','zipcode' , 'first_review' , 'last_review' , 'neighbourhood' , 'city']\n",
    "df_dropped = df.drop(columns=cols_to_drop, axis = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = ['cancellation_policy', 'property_type', 'room_type']\n",
    "df_with_dummies = pd.get_dummies(df_dropped, columns=categorical_columns, drop_first=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scores = {}\n",
    "X = df_with_dummies.drop(columns=['log_price'])\n",
    "y = df_with_dummies['log_price']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_name = 'model_lr_TUNED.joblib'\n",
    "\n",
    "param_grid = {\n",
    "}\n",
    "\n",
    "if os.path.exists(saved_model_name):\n",
    "    loaded_model = joblib.load(saved_model_name)\n",
    "    model_lr_TUNED = loaded_model\n",
    "else:\n",
    "    grid_search = GridSearchCV(LinearRegression(), param_grid, cv=5 , n_jobs = -1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    model_lr_TUNED = grid_search.best_estimator_\n",
    "    joblib.dump(model_lr_TUNED, saved_model_name)\n",
    "\n",
    "y_pred = model_lr_TUNED.predict(X_test)\n",
    "\n",
    "train_score = model_lr_TUNED.score(X_train, y_train)\n",
    "test_score = model_lr_TUNED.score(X_test, y_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "params = model_lr_TUNED.get_params()\n",
    "\n",
    "model_scores['Linear Regression TUNED'] = {\n",
    "    'Train Score': train_score,\n",
    "    'Test Score': test_score,\n",
    "    'Used Parameters': params\n",
    "}\n",
    "\n",
    "print(f\"Train Score (R²): {train_score}\")\n",
    "print(f\"Test Score (R²): {test_score}\")\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R² Score: {r2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_name_ridge = 'model_ridge_TUNED.joblib'\n",
    "saved_model_name_lasso = 'model_lasso_TUNED.joblib'\n",
    "\n",
    "param_grid_ridge = {\n",
    "    'alpha': [0.01, 0.1, 1, 10, 100],\n",
    "    'fit_intercept': [True, False]\n",
    "}\n",
    "\n",
    "param_grid_lasso = {\n",
    "    'alpha': [0.01, 0.1, 1, 10, 100],\n",
    "    'fit_intercept': [True, False]\n",
    "}\n",
    "\n",
    "# Ridge Regression\n",
    "if os.path.exists(saved_model_name_ridge):\n",
    "    model_ridge_TUNED = joblib.load(saved_model_name_ridge)\n",
    "else:\n",
    "    grid_search_ridge = GridSearchCV(Ridge(), param_grid_ridge, cv=5 , n_jobs = -1)\n",
    "    grid_search_ridge.fit(X_train, y_train)\n",
    "    model_ridge_TUNED = grid_search_ridge.best_estimator_\n",
    "    joblib.dump(model_ridge_TUNED, saved_model_name_ridge)\n",
    "\n",
    "y_pred_ridge = model_ridge_TUNED.predict(X_test)\n",
    "\n",
    "train_score_ridge = model_ridge_TUNED.score(X_train, y_train)\n",
    "test_score_ridge = model_ridge_TUNED.score(X_test, y_test)\n",
    "mse_ridge = mean_squared_error(y_test, y_pred_ridge)\n",
    "r2_ridge = r2_score(y_test, y_pred_ridge)\n",
    "\n",
    "params_ridge = model_ridge_TUNED.get_params()\n",
    "\n",
    "model_scores['Ridge Regression TUNED'] = {\n",
    "    'Train Score': train_score_ridge,\n",
    "    'Test Score': test_score_ridge,\n",
    "    'Mean Squared Error': mse_ridge,\n",
    "    'R2 Score': r2_ridge,\n",
    "    'Used Parameters': params_ridge\n",
    "}\n",
    "\n",
    "print(f\"Ridge Regression - Train Score (R²): {train_score_ridge}\")\n",
    "print(f\"Ridge Regression - Test Score (R²): {test_score_ridge}\")\n",
    "print(f\"Ridge Regression - Mean Squared Error: {mse_ridge}\")\n",
    "print(f\"Ridge Regression - R² Score: {r2_ridge}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(saved_model_name_lasso):\n",
    "    model_lasso_TUNED = joblib.load(saved_model_name_lasso)\n",
    "else:\n",
    "    grid_search_lasso = GridSearchCV(Lasso(), param_grid_lasso, cv=5 , n_jobs = -1)\n",
    "    grid_search_lasso.fit(X_train, y_train)\n",
    "    model_lasso_TUNED = grid_search_lasso.best_estimator_\n",
    "    joblib.dump(model_lasso_TUNED, saved_model_name_lasso)\n",
    "\n",
    "y_pred_lasso = model_lasso_TUNED.predict(X_test)\n",
    "\n",
    "train_score_lasso = model_lasso_TUNED.score(X_train, y_train)\n",
    "test_score_lasso = model_lasso_TUNED.score(X_test, y_test)\n",
    "mse_lasso = mean_squared_error(y_test, y_pred_lasso)\n",
    "r2_lasso = r2_score(y_test, y_pred_lasso)\n",
    "\n",
    "params_lasso = model_lasso_TUNED.get_params()\n",
    "\n",
    "model_scores['Lasso Regression TUNED'] = {\n",
    "    'Train Score': train_score_lasso,\n",
    "    'Test Score': test_score_lasso,\n",
    "    'Used Parameters': params_lasso\n",
    "}\n",
    "\n",
    "print(f\"Lasso Regression - Train Score (R²): {train_score_lasso}\")\n",
    "print(f\"Lasso Regression - Test Score (R²): {test_score_lasso}\")\n",
    "print(f\"Lasso Regression - Mean Squared Error: {mse_lasso}\")\n",
    "print(f\"Lasso Regression - R² Score: {r2_lasso}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_name_elastic = 'model_elastic_TUNED.joblib'\n",
    "\n",
    "param_grid_elastic = {\n",
    "    'alpha': [0.01, 0.1, 1, 10, 100],\n",
    "    'l1_ratio': [0.1, 0.5, 0.7, 0.9],\n",
    "    'fit_intercept': [True, False]\n",
    "}\n",
    "\n",
    "# Elastic Net Regression\n",
    "if os.path.exists(saved_model_name_elastic):\n",
    "    model_elastic_TUNED = joblib.load(saved_model_name_elastic)\n",
    "else:\n",
    "    grid_search_elastic = GridSearchCV(ElasticNet(), param_grid_elastic, cv=5 , n_jobs = -1)\n",
    "    grid_search_elastic.fit(X_train, y_train)\n",
    "    model_elastic_TUNED = grid_search_elastic.best_estimator_\n",
    "    joblib.dump(model_elastic_TUNED, saved_model_name_elastic)\n",
    "\n",
    "y_pred_elastic = model_elastic_TUNED.predict(X_test)\n",
    "\n",
    "train_score_elastic = model_elastic_TUNED.score(X_train, y_train)\n",
    "test_score_elastic = model_elastic_TUNED.score(X_test, y_test)\n",
    "mse_elastic = mean_squared_error(y_test, y_pred_elastic)\n",
    "r2_elastic = r2_score(y_test, y_pred_elastic)\n",
    "\n",
    "params_elastic = model_elastic_TUNED.get_params()\n",
    "alpha_used = params_elastic['alpha']\n",
    "l1_ratio_used = params_elastic['l1_ratio']\n",
    "\n",
    "model_scores['Elastic Net Regression TUNED'] = {\n",
    "    'Train Score': train_score_elastic,\n",
    "    'Test Score': test_score_elastic,\n",
    "    'Used Parameters': params_elastic\n",
    "}\n",
    "\n",
    "print(f\"Elastic Net Regression - Train Score (R²): {train_score_elastic}\")\n",
    "print(f\"Elastic Net Regression - Test Score (R²): {test_score_elastic}\")\n",
    "print(f\"Elastic Net Regression - Mean Squared Error: {mse_elastic}\")\n",
    "print(f\"Elastic Net Regression - R² Score: {r2_elastic}\")\n",
    "print(f\"Elastic Net Regression - Lambda (Alpha): {alpha_used}\")\n",
    "print(f\"Elastic Net Regression - L1 Ratio: {l1_ratio_used}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_name_dt = 'model_dt_TUNED.joblib'\n",
    "\n",
    "param_grid_dt = {\n",
    "    'max_depth': [3, 5, 10, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 5],\n",
    "    'criterion': ['mse', 'friedman_mse', 'mae']\n",
    "}\n",
    "\n",
    "if os.path.exists(saved_model_name_dt):\n",
    "    model_dt_TUNED = joblib.load(saved_model_name_dt)\n",
    "else:\n",
    "    grid_search_dt = GridSearchCV(DecisionTreeRegressor(), param_grid_dt, cv=5 , n_jobs = -1)\n",
    "    grid_search_dt.fit(X_train, y_train)\n",
    "    model_dt_TUNED = grid_search_dt.best_estimator_\n",
    "    joblib.dump(model_dt_TUNED, saved_model_name_dt)\n",
    "\n",
    "y_pred_dt = model_dt_TUNED.predict(X_test)\n",
    "\n",
    "train_score_dt = model_dt_TUNED.score(X_train, y_train)\n",
    "test_score_dt = model_dt_TUNED.score(X_test, y_test)\n",
    "mse_dt = mean_squared_error(y_test, y_pred_dt)\n",
    "r2_dt = r2_score(y_test, y_pred_dt)\n",
    "\n",
    "params_dt = model_dt_TUNED.get_params()\n",
    "\n",
    "model_scores['Decision Tree Regression TUNED'] = {\n",
    "    'Train Score': train_score_dt,\n",
    "    'Test Score': test_score_dt,\n",
    "    'Used Parameters': params_dt\n",
    "}\n",
    "\n",
    "print(f\"Decision Tree Regression - Train Score (R²): {train_score_dt}\")\n",
    "print(f\"Decision Tree Regression - Test Score (R²): {test_score_dt}\")\n",
    "print(f\"Decision Tree Regression - Mean Squared Error: {mse_dt}\")\n",
    "print(f\"Decision Tree Regression - R² Score: {r2_dt}\")\n",
    "print(f\"Decision Tree Regression - Best Parameters: {params_dt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_name_rf = 'model_rf_TUNED.joblib'\n",
    "\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "}\n",
    "\n",
    "if os.path.exists(saved_model_name_rf):\n",
    "    model_rf_TUNED = joblib.load(saved_model_name_rf)\n",
    "else:\n",
    "    grid_search_rf = GridSearchCV(RandomForestRegressor(), param_grid_rf, cv=5 , n_jobs = -1)\n",
    "    grid_search_rf.fit(X_train, y_train)\n",
    "    model_rf_TUNED = grid_search_rf.best_estimator_\n",
    "    joblib.dump(model_rf_TUNED, saved_model_name_rf)\n",
    "\n",
    "y_pred_rf = model_rf_TUNED.predict(X_test)\n",
    "\n",
    "train_score_rf = model_rf_TUNED.score(X_train, y_train)\n",
    "test_score_rf = model_rf_TUNED.score(X_test, y_test)\n",
    "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
    "r2_rf = r2_score(y_test, y_pred_rf)\n",
    "\n",
    "params_rf = model_rf_TUNED.get_params()\n",
    "\n",
    "model_scores['Random Forest Regression TUNED'] = {\n",
    "    'Train Score': train_score_rf,\n",
    "    'Test Score': test_score_rf,\n",
    "    'Used Parameters': params_rf\n",
    "}\n",
    "\n",
    "print(f\"Random Forest Regression - Train Score (R²): {train_score_rf}\")\n",
    "print(f\"Random Forest Regression - Test Score (R²): {test_score_rf}\")\n",
    "print(f\"Random Forest Regression - Mean Squared Error: {mse_rf}\")\n",
    "print(f\"Random Forest Regression - R² Score: {r2_rf}\")\n",
    "print(f\"Random Forest Regression - Best Parameters: {params_rf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_name_gb = 'model_gb_TUNED.joblib'\n",
    "\n",
    "param_grid_gb = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "if os.path.exists(saved_model_name_gb):\n",
    "    model_gb_TUNED = joblib.load(saved_model_name_gb)\n",
    "else:\n",
    "    grid_search_gb = GridSearchCV(GradientBoostingRegressor(), param_grid_gb, cv=5 , n_jobs = -1)\n",
    "    grid_search_gb.fit(X_train, y_train)\n",
    "    model_gb_TUNED = grid_search_gb.best_estimator_\n",
    "    joblib.dump(model_gb_TUNED, saved_model_name_gb)\n",
    "\n",
    "y_pred_gb = model_gb_TUNED.predict(X_test)\n",
    "\n",
    "train_score_gb = model_gb_TUNED.score(X_train, y_train)\n",
    "test_score_gb = model_gb_TUNED.score(X_test, y_test)\n",
    "mse_gb = mean_squared_error(y_test, y_pred_gb)\n",
    "r2_gb = r2_score(y_test, y_pred_gb)\n",
    "\n",
    "params_gb = model_gb_TUNED.get_params()\n",
    "\n",
    "model_scores['Gradient Boosting Regression TUNED'] = {\n",
    "    'Train Score': train_score_gb,\n",
    "    'Test Score': test_score_gb,\n",
    "    'Used Parameters': params_gb\n",
    "}\n",
    "\n",
    "print(f\"Gradient Boosting Regression - Train Score (R²): {train_score_gb}\")\n",
    "print(f\"Gradient Boosting Regression - Test Score (R²): {test_score_gb}\")\n",
    "print(f\"Gradient Boosting Regression - Mean Squared Error: {mse_gb}\")\n",
    "print(f\"Gradient Boosting Regression - R² Score: {r2_gb}\")\n",
    "print(f\"Gradient Boosting Regression - Best Parameters: {params_gb}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_name_xgb = 'model_xgb_TUNED.joblib'\n",
    "\n",
    "param_grid_xgb = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'min_child_weight': [1, 3, 5],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0],\n",
    "    'gamma': [0, 0.1, 0.2]\n",
    "}\n",
    "\n",
    "if os.path.exists(saved_model_name_xgb):\n",
    "    model_xgb_TUNED = joblib.load(saved_model_name_xgb)\n",
    "else:\n",
    "    grid_search_xgb = GridSearchCV(XGBRegressor(), param_grid_xgb, cv=5, n_jobs=-1, scoring='r2')\n",
    "    grid_search_xgb.fit(X_train, y_train)\n",
    "    model_xgb_TUNED = grid_search_xgb.best_estimator_\n",
    "    joblib.dump(model_xgb_TUNED, saved_model_name_xgb)\n",
    "\n",
    "y_pred_xgb = model_xgb_TUNED.predict(X_test)\n",
    "\n",
    "train_score_xgb = model_xgb_TUNED.score(X_train, y_train)\n",
    "test_score_xgb = model_xgb_TUNED.score(X_test, y_test)\n",
    "mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
    "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
    "\n",
    "params_xgb = model_xgb_TUNED.get_params()\n",
    "\n",
    "model_scores['XGBoost Regression TUNED'] = {\n",
    "    'Train Score': train_score_xgb,\n",
    "    'Test Score': test_score_xgb,\n",
    "    'Used Parameters': params_xgb\n",
    "}\n",
    "\n",
    "# Print the results\n",
    "print(f\"XGBoost Regression - Train Score (R²): {train_score_xgb}\")\n",
    "print(f\"XGBoost Regression - Test Score (R²): {test_score_xgb}\")\n",
    "print(f\"XGBoost Regression - Mean Squared Error: {mse_xgb}\")\n",
    "print(f\"XGBoost Regression - R² Score: {r2_xgb}\")\n",
    "print(f\"XGBoost Regression - Best Parameters: {params_xgb}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = list(model_scores.keys())\n",
    "train_scores = [model_scores[model][\"Train Score\"] for model in models]\n",
    "test_scores = [model_scores[model][\"Test Score\"] for model in models]\n",
    "\n",
    "x = range(len(models))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(x, train_scores, width=0.4, label='Train Score', color='b', align='center')\n",
    "plt.bar([p + 0.4 for p in x], test_scores, width=0.4, label='Test Score', color='orange', align='center')\n",
    "\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('Scores')\n",
    "plt.title('Train and Test Scores of Different Models')\n",
    "plt.xticks([p + 0.2 for p in x], models , rotation =90)\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = max(model_scores, key=lambda x: model_scores[x]['Test Score'])\n",
    "print('the best model -', best_model)\n",
    "print(model_scores[best_model]['Train Score']) \n",
    "print(model_scores[best_model]['Test Score'])\n",
    "\n",
    "worst_model = min(model_scores, key=lambda x: model_scores[x]['Test Score'])\n",
    "print('the worst model -', worst_model)\n",
    "print(model_scores[worst_model]['Train Score']) \n",
    "print(model_scores[worst_model]['Test Score'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
