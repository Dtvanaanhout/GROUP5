{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, DecisionTreeRegressor, KNeighborsRegressor, RandomForestRegressor, GradientBoostingRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "model_scores = {}\n",
    "X = df_with_dummies.drop(columns=['log_price])\n",
    "y = df_with_dummies['log_price']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "saved_model_name = 'model_lr_TUNED.joblib'\n",
    "\n",
    "param_grid = {\n",
    "    'fit_intercept': [True, False],\n",
    "    'normalize': [True, False]\n",
    "}\n",
    "\n",
    "if os.path.exists(saved_model_name):\n",
    "    loaded_model = joblib.load(saved_model_name)\n",
    "    model_lr_TUNED = loaded_model\n",
    "else:\n",
    "    grid_search = GridSearchCV(LinearRegression(), param_grid, cv=5)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    model_lr_TUNED = grid_search.best_estimator_\n",
    "    joblib.dump(model_lr_TUNED, saved_model_name)\n",
    "\n",
    "y_pred = model_lr_TUNED.predict(X_test)\n",
    "\n",
    "train_score = model_lr_TUNED.score(X_train, y_train)\n",
    "test_score = model_lr_TUNED.score(X_test, y_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "params = model_lr_TUNED.get_params()\n",
    "\n",
    "model_scores['Linear Regression TUNED'] = {\n",
    "    'Train Score': train_score,\n",
    "    'Test Score': test_score,\n",
    "    'Mean Squared Error': mse,\n",
    "    'R2 Score': r2,\n",
    "    'Used Parameters': params\n",
    "}\n",
    "\n",
    "print(f\"Train Score (R²): {train_score}\")\n",
    "print(f\"Test Score (R²): {test_score}\")\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R² Score: {r2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "saved_model_name_knn = 'model_knn_TUNED.joblib'\n",
    "\n",
    "param_grid_knn = {\n",
    "    'n_neighbors': [3, 5, 7, 10, 15],\n",
    "    'p': [1, 2]  \n",
    "}\n",
    "\n",
    "# KNN Regression with GridSearchCV\n",
    "if os.path.exists(saved_model_name_knn):\n",
    "    model_knn_TUNED = joblib.load(saved_model_name_knn)\n",
    "else:\n",
    "    grid_search_knn = GridSearchCV(KNeighborsRegressor(), param_grid_knn, cv=5)\n",
    "    grid_search_knn.fit(X_train, y_train)\n",
    "    model_knn_TUNED = grid_search_knn.best_estimator_\n",
    "    joblib.dump(model_knn_TUNED, saved_model_name_knn)\n",
    "\n",
    "y_pred_knn = model_knn_TUNED.predict(X_test)\n",
    "\n",
    "train_score_knn = model_knn_TUNED.score(X_train, y_train)\n",
    "test_score_knn = model_knn_TUNED.score(X_test, y_test)\n",
    "mse_knn = mean_squared_error(y_test, y_pred_knn)\n",
    "r2_knn = r2_score(y_test, y_pred_knn)\n",
    "\n",
    "params_knn = model_knn_TUNED.get_params()\n",
    "\n",
    "model_scores['KNN Regression TUNED'] = {\n",
    "    'Train Score': train_score_knn,\n",
    "    'Test Score': test_score_knn,\n",
    "    'Mean Squared Error': mse_knn,\n",
    "    'R² Score': r2_knn,\n",
    "    'Used Parameters': params_knn\n",
    "}\n",
    "\n",
    "print(f\"KNN Regression - Train Score (R²): {train_score_knn}\")\n",
    "print(f\"KNN Regression - Test Score (R²): {test_score_knn}\")\n",
    "print(f\"KNN Regression - Mean Squared Error: {mse_knn}\")\n",
    "print(f\"KNN Regression - R² Score: {r2_knn}\")\n",
    "print(f\"KNN Regression - Best Parameters: {params_knn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "saved_model_name_ridge = 'model_ridge_TUNED.joblib'\n",
    "saved_model_name_lasso = 'model_lasso_TUNED.joblib'\n",
    "\n",
    "param_grid_ridge = {\n",
    "    'alpha': [0.01, 0.1, 1, 10, 100],\n",
    "    'fit_intercept': [True, False]\n",
    "}\n",
    "\n",
    "param_grid_lasso = {\n",
    "    'alpha': [0.01, 0.1, 1, 10, 100],\n",
    "    'fit_intercept': [True, False]\n",
    "}\n",
    "\n",
    "# Ridge Regression\n",
    "if os.path.exists(saved_model_name_ridge):\n",
    "    model_ridge_TUNED = joblib.load(saved_model_name_ridge)\n",
    "else:\n",
    "    grid_search_ridge = GridSearchCV(Ridge(), param_grid_ridge, cv=5)\n",
    "    grid_search_ridge.fit(X_train, y_train)\n",
    "    model_ridge_TUNED = grid_search_ridge.best_estimator_\n",
    "    joblib.dump(model_ridge_TUNED, saved_model_name_ridge)\n",
    "\n",
    "y_pred_ridge = model_ridge_TUNED.predict(X_test)\n",
    "\n",
    "train_score_ridge = model_ridge_TUNED.score(X_train, y_train)\n",
    "test_score_ridge = model_ridge_TUNED.score(X_test, y_test)\n",
    "mse_ridge = mean_squared_error(y_test, y_pred_ridge)\n",
    "r2_ridge = r2_score(y_test, y_pred_ridge)\n",
    "\n",
    "params_ridge = model_ridge_TUNED.get_params()\n",
    "\n",
    "model_scores['Ridge Regression TUNED'] = {\n",
    "    'Train Score': train_score_ridge,\n",
    "    'Test Score': test_score_ridge,\n",
    "    'Mean Squared Error': mse_ridge,\n",
    "    'R2 Score': r2_ridge,\n",
    "    'Used Parameters': params_ridge\n",
    "}\n",
    "\n",
    "print(f\"Ridge Regression - Train Score (R²): {train_score_ridge}\")\n",
    "print(f\"Ridge Regression - Test Score (R²): {test_score_ridge}\")\n",
    "print(f\"Ridge Regression - Mean Squared Error: {mse_ridge}\")\n",
    "print(f\"Ridge Regression - R² Score: {r2_ridge}\")\n",
    "\n",
    "# Lasso Regression\n",
    "if os.path.exists(saved_model_name_lasso):\n",
    "    model_lasso_TUNED = joblib.load(saved_model_name_lasso)\n",
    "else:\n",
    "    grid_search_lasso = GridSearchCV(Lasso(), param_grid_lasso, cv=5)\n",
    "    grid_search_lasso.fit(X_train, y_train)\n",
    "    model_lasso_TUNED = grid_search_lasso.best_estimator_\n",
    "    joblib.dump(model_lasso_TUNED, saved_model_name_lasso)\n",
    "\n",
    "y_pred_lasso = model_lasso_TUNED.predict(X_test)\n",
    "\n",
    "train_score_lasso = model_lasso_TUNED.score(X_train, y_train)\n",
    "test_score_lasso = model_lasso_TUNED.score(X_test, y_test)\n",
    "mse_lasso = mean_squared_error(y_test, y_pred_lasso)\n",
    "r2_lasso = r2_score(y_test, y_pred_lasso)\n",
    "\n",
    "params_lasso = model_lasso_TUNED.get_params()\n",
    "\n",
    "model_scores['Lasso Regression TUNED'] = {\n",
    "    'Train Score': train_score_lasso,\n",
    "    'Test Score': test_score_lasso,\n",
    "    'Mean Squared Error': mse_lasso,\n",
    "    'R2 Score': r2_lasso,\n",
    "    'Used Parameters': params_lasso\n",
    "}\n",
    "\n",
    "print(f\"Lasso Regression - Train Score (R²): {train_score_lasso}\")\n",
    "print(f\"Lasso Regression - Test Score (R²): {test_score_lasso}\")\n",
    "print(f\"Lasso Regression - Mean Squared Error: {mse_lasso}\")\n",
    "print(f\"Lasso Regression - R² Score: {r2_lasso}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "saved_model_name_elastic = 'model_elastic_TUNED.joblib'\n",
    "\n",
    "param_grid_elastic = {\n",
    "    'alpha': [0.01, 0.1, 1, 10, 100],\n",
    "    'l1_ratio': [0.1, 0.5, 0.7, 0.9],\n",
    "    'fit_intercept': [True, False]\n",
    "}\n",
    "\n",
    "# Elastic Net Regression\n",
    "if os.path.exists(saved_model_name_elastic):\n",
    "    model_elastic_TUNED = joblib.load(saved_model_name_elastic)\n",
    "else:\n",
    "    grid_search_elastic = GridSearchCV(ElasticNet(), param_grid_elastic, cv=5)\n",
    "    grid_search_elastic.fit(X_train, y_train)\n",
    "    model_elastic_TUNED = grid_search_elastic.best_estimator_\n",
    "    joblib.dump(model_elastic_TUNED, saved_model_name_elastic)\n",
    "\n",
    "y_pred_elastic = model_elastic_TUNED.predict(X_test)\n",
    "\n",
    "train_score_elastic = model_elastic_TUNED.score(X_train, y_train)\n",
    "test_score_elastic = model_elastic_TUNED.score(X_test, y_test)\n",
    "mse_elastic = mean_squared_error(y_test, y_pred_elastic)\n",
    "r2_elastic = r2_score(y_test, y_pred_elastic)\n",
    "\n",
    "params_elastic = model_elastic_TUNED.get_params()\n",
    "alpha_used = params_elastic['alpha']\n",
    "l1_ratio_used = params_elastic['l1_ratio']\n",
    "\n",
    "model_scores['Elastic Net Regression TUNED'] = {\n",
    "    'Train Score': train_score_elastic,\n",
    "    'Test Score': test_score_elastic,\n",
    "    'Mean Squared Error': mse_elastic,\n",
    "    'R² Score': r2_elastic,\n",
    "    'Lambda (Alpha)': alpha_used,\n",
    "    'L1 Ratio': l1_ratio_used,\n",
    "    'Used Parameters': params_elastic\n",
    "}\n",
    "\n",
    "print(f\"Elastic Net Regression - Train Score (R²): {train_score_elastic}\")\n",
    "print(f\"Elastic Net Regression - Test Score (R²): {test_score_elastic}\")\n",
    "print(f\"Elastic Net Regression - Mean Squared Error: {mse_elastic}\")\n",
    "print(f\"Elastic Net Regression - R² Score: {r2_elastic}\")\n",
    "print(f\"Elastic Net Regression - Lambda (Alpha): {alpha_used}\")\n",
    "print(f\"Elastic Net Regression - L1 Ratio: {l1_ratio_used}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "saved_model_name_dt = 'model_dt_TUNED.joblib'\n",
    "\n",
    "param_grid_dt = {\n",
    "    'max_depth': [3, 5, 10, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 5],\n",
    "    'criterion': ['mse', 'friedman_mse', 'mae']\n",
    "}\n",
    "\n",
    "# Decision Tree Regression with GridSearchCV\n",
    "if os.path.exists(saved_model_name_dt):\n",
    "    model_dt_TUNED = joblib.load(saved_model_name_dt)\n",
    "else:\n",
    "    grid_search_dt = GridSearchCV(DecisionTreeRegressor(), param_grid_dt, cv=5)\n",
    "    grid_search_dt.fit(X_train, y_train)\n",
    "    model_dt_TUNED = grid_search_dt.best_estimator_\n",
    "    joblib.dump(model_dt_TUNED, saved_model_name_dt)\n",
    "\n",
    "y_pred_dt = model_dt_TUNED.predict(X_test)\n",
    "\n",
    "train_score_dt = model_dt_TUNED.score(X_train, y_train)\n",
    "test_score_dt = model_dt_TUNED.score(X_test, y_test)\n",
    "mse_dt = mean_squared_error(y_test, y_pred_dt)\n",
    "r2_dt = r2_score(y_test, y_pred_dt)\n",
    "\n",
    "params_dt = model_dt_TUNED.get_params()\n",
    "\n",
    "model_scores['Decision Tree Regression TUNED'] = {\n",
    "    'Train Score': train_score_dt,\n",
    "    'Test Score': test_score_dt,\n",
    "    'Mean Squared Error': mse_dt,\n",
    "    'R² Score': r2_dt,\n",
    "    'Used Parameters': params_dt\n",
    "}\n",
    "\n",
    "print(f\"Decision Tree Regression - Train Score (R²): {train_score_dt}\")\n",
    "print(f\"Decision Tree Regression - Test Score (R²): {test_score_dt}\")\n",
    "print(f\"Decision Tree Regression - Mean Squared Error: {mse_dt}\")\n",
    "print(f\"Decision Tree Regression - R² Score: {r2_dt}\")\n",
    "print(f\"Decision Tree Regression - Best Parameters: {params_dt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "saved_model_name_rf = 'model_rf_TUNED.joblib'\n",
    "\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "}\n",
    "\n",
    "# Random Forest Regression with GridSearchCV\n",
    "if os.path.exists(saved_model_name_rf):\n",
    "    model_rf_TUNED = joblib.load(saved_model_name_rf)\n",
    "else:\n",
    "    grid_search_rf = GridSearchCV(RandomForestRegressor(), param_grid_rf, cv=5)\n",
    "    grid_search_rf.fit(X_train, y_train)\n",
    "    model_rf_TUNED = grid_search_rf.best_estimator_\n",
    "    joblib.dump(model_rf_TUNED, saved_model_name_rf)\n",
    "\n",
    "y_pred_rf = model_rf_TUNED.predict(X_test)\n",
    "\n",
    "train_score_rf = model_rf_TUNED.score(X_train, y_train)\n",
    "test_score_rf = model_rf_TUNED.score(X_test, y_test)\n",
    "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
    "r2_rf = r2_score(y_test, y_pred_rf)\n",
    "\n",
    "params_rf = model_rf_TUNED.get_params()\n",
    "\n",
    "model_scores['Random Forest Regression TUNED'] = {\n",
    "    'Train Score': train_score_rf,\n",
    "    'Test Score': test_score_rf,\n",
    "    'Mean Squared Error': mse_rf,\n",
    "    'R² Score': r2_rf,\n",
    "    'Used Parameters': params_rf\n",
    "}\n",
    "\n",
    "print(f\"Random Forest Regression - Train Score (R²): {train_score_rf}\")\n",
    "print(f\"Random Forest Regression - Test Score (R²): {test_score_rf}\")\n",
    "print(f\"Random Forest Regression - Mean Squared Error: {mse_rf}\")\n",
    "print(f\"Random Forest Regression - R² Score: {r2_rf}\")\n",
    "print(f\"Random Forest Regression - Best Parameters: {params_rf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "saved_model_name_gb = 'model_gb_TUNED.joblib'\n",
    "\n",
    "param_grid_gb = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Gradient Boosting Regression with GridSearchCV\n",
    "if os.path.exists(saved_model_name_gb):\n",
    "    model_gb_TUNED = joblib.load(saved_model_name_gb)\n",
    "else:\n",
    "    grid_search_gb = GridSearchCV(GradientBoostingRegressor(), param_grid_gb, cv=5)\n",
    "    grid_search_gb.fit(X_train, y_train)\n",
    "    model_gb_TUNED = grid_search_gb.best_estimator_\n",
    "    joblib.dump(model_gb_TUNED, saved_model_name_gb)\n",
    "\n",
    "y_pred_gb = model_gb_TUNED.predict(X_test)\n",
    "\n",
    "train_score_gb = model_gb_TUNED.score(X_train, y_train)\n",
    "test_score_gb = model_gb_TUNED.score(X_test, y_test)\n",
    "mse_gb = mean_squared_error(y_test, y_pred_gb)\n",
    "r2_gb = r2_score(y_test, y_pred_gb)\n",
    "\n",
    "params_gb = model_gb_TUNED.get_params()\n",
    "\n",
    "model_scores['Gradient Boosting Regression TUNED'] = {\n",
    "    'Train Score': train_score_gb,\n",
    "    'Test Score': test_score_gb,\n",
    "    'Mean Squared Error': mse_gb,\n",
    "    'R² Score': r2_gb,\n",
    "    'Used Parameters': params_gb\n",
    "}\n",
    "\n",
    "print(f\"Gradient Boosting Regression - Train Score (R²): {train_score_gb}\")\n",
    "print(f\"Gradient Boosting Regression - Test Score (R²): {test_score_gb}\")\n",
    "print(f\"Gradient Boosting Regression - Mean Squared Error: {mse_gb}\")\n",
    "print(f\"Gradient Boosting Regression - R² Score: {r2_gb}\")\n",
    "print(f\"Gradient Boosting Regression - Best Parameters: {params_gb}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "models = list(model_scores.keys())\n",
    "train_scores = [model_scores[model][\"Train Score\"] for model in models]\n",
    "test_scores = [model_scores[model][\"Test Score\"] for model in models]\n",
    "\n",
    "x = range(len(models))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(x, train_scores, width=0.4, label='Train Score', color='b', align='center')\n",
    "plt.bar([p + 0.4 for p in x], test_scores, width=0.4, label='Test Score', color='orange', align='center')\n",
    "\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('Scores')\n",
    "plt.title('Train and Test Scores of Different Models')\n",
    "plt.xticks([p + 0.2 for p in x], models , rotation =90)\n",
    "plt.ylim(0.5, 1)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "best_model = max(model_scores, key=lambda x: model_scores[x]['Test Score'])\n",
    "print('the best model -', best_model)\n",
    "print(model_scores[best_model]['Train Score']) \n",
    "print(model_scores[best_model]['Test Score'])\n",
    "print(model_scores[best_model]['Used parameters']) \n",
    "\n",
    "worst_model = min(model_scores, key=lambda x: model_scores[x]['Test Score'])\n",
    "print('the worst model -', worst_model)\n",
    "print(model_scores[worst_model]['Train Score']) \n",
    "print(model_scores[worst_model]['Test Score'])\n",
    "print(model_scores[worst_model]['Used parameters']) "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
