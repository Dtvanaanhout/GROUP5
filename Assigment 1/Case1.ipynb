{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "xZEzN0FhAXcY"
   },
   "source": [
    "# **The Problem & Business Importance**\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "MHWy8k7SAZLx"
   },
   "source": [
    "# **Data Identification & Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "plINWFh7Bgh5"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtree\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DecisionTreeClassifier\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msvm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SVC\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mxgboost\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m XGBClassifier\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "from KNN_D import KNN_D\n",
    "\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV,train_test_split , KFold\n",
    "from sklearn.linear_model import LogisticRegression , RidgeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f6Jvdrwm5OTq"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('i4talent_dataset.csv')\n",
    "df['datum'] = pd.to_datetime(df['datum'])\n",
    "df['geboortedatum'] = pd.to_datetime(df['geboortedatum'])\n",
    "df['indiensttreding_datum'] = pd.to_datetime(df['indiensttreding_datum'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "JXnHTqgh5WGp",
    "outputId": "72520840-948d-446f-d6d6-1017168ac6be"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "VKwqRLC-5_IZ",
    "outputId": "89956ba4-116f-4b0f-eb74-70161c7bc364"
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fbDDKRwf7Y_d",
    "outputId": "42a86b64-0b52-4eec-b3f0-4f420d1080e4"
   },
   "outputs": [],
   "source": [
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7omAH42L8a2D",
    "outputId": "87eb756c-c506-4ccb-b41f-676e241a2f3d"
   },
   "outputs": [],
   "source": [
    "df['leeftijd'] = df['leeftijd'].fillna((df['datum'] - df['geboortedatum']).dt.days // 365)\n",
    "df['lengte_dienst'] = df['lengte_dienst'].fillna((df['datum'] - df['indiensttreding_datum']).dt.days // 365)\n",
    "df['stad'] = df['stad'].fillna(df['stad'].mode())\n",
    "df['afdeling'] = df['afdeling'].fillna(df['afdeling'].mode())\n",
    "\n",
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 927
    },
    "id": "4pN76r4P8ip_",
    "outputId": "59b58995-917d-476e-f78a-961e868e8886"
   },
   "outputs": [],
   "source": [
    "columns_numerical_with_nas = ['leeftijd', 'lengte_dienst']\n",
    "for col in columns_numerical_with_nas:\n",
    "    plt.hist(df[col], bins=100)\n",
    "    plt.title(col)\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "iCGSj9AbGNIL",
    "outputId": "1a61ed89-4487-46e3-aa41-1d2894b3fb06"
   },
   "outputs": [],
   "source": [
    "columns_to_delete = ['geboortedatum', 'WerknemerID', 'uitdiensttreding_datum', 'indiensttreding_datum', 'geslacht', 'uitdiensttreding_type', 'datum', 'STATUS_JAAR', 'uitdiensttreding_reden']\n",
    "df_new = df.drop(columns=columns_to_delete)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "id": "W4hkCTGFI89z",
    "outputId": "72b1fb39-7491-4a86-cd5f-a7c0970160d2"
   },
   "outputs": [],
   "source": [
    "df_with_dummies = pd.get_dummies(df_new, columns=['stad', 'afdeling', 'geslacht_id', 'STATUS', 'BUSINESS_UNIT'], drop_first=True , dtype = int)\n",
    "df_with_dummies.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "n2OVALsLKODq"
   },
   "source": [
    "geboortedatum, WerknemerID, uitdiensttreding_datum, indiensttreding_datum, geslacht, uitdiensttreding_type"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "RBA427i_KbZD"
   },
   "source": [
    "Dummies: 'stad', 'afdeling', 'geslachtID', 'uitdiensttreding_reden', 'Status', 'BUSINESS_UNIT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2nGpDbGgP2eK"
   },
   "outputs": [],
   "source": [
    "#Used later for model evaluation\n",
    "model_scores = {}\n",
    "\n",
    "#You can download the pretrained models here : https://github.com/dvanaanhout/GROUP5\n",
    "#Saves time running the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_with_dummies.drop(columns=['STATUS_Beëindigd'])\n",
    "y = df_with_dummies['STATUS_Beëindigd']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KLPPLuhchdar",
    "outputId": "f820ae07-b842-4abe-ca6f-808e939c7431"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "# Load or Train Logistic Regression Model\n",
    "saved_model_name = 'model_lr.joblib'\n",
    "\n",
    "if os.path.exists(saved_model_name):\n",
    "    loaded_model = joblib.load(saved_model_name)\n",
    "    model_lr = loaded_model\n",
    "else:\n",
    "    model_lr = LogisticRegression()\n",
    "    model_lr.fit(X_train, y_train)\n",
    "    joblib.dump(model_lr, saved_model_name)\n",
    "\n",
    "# Predictions and Model Scores\n",
    "y_pred = model_lr.predict(X_test)\n",
    "\n",
    "train_score = model_lr.score(X_train, y_train)\n",
    "test_score = model_lr.score(X_test, y_test)\n",
    "\n",
    "model_scores['Logistic Regression'] = {\n",
    "    'Train Score': train_score,\n",
    "    'Test Score': test_score\n",
    "}\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(f\"Train Score: {train_score}\")\n",
    "print(f\"Test Score: {test_score}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Feature Importance\n",
    "coefficients = model_lr.coef_[0]\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Coefficient': coefficients,\n",
    "    'Importance': np.abs(coefficients)\n",
    "})\n",
    "\n",
    "feature_importance = feature_importance.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance['Feature'], feature_importance['Importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Feature Importance (Logistic Regression)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Multicollinearity Check\n",
    "# Add a constant to X_train to calculate VIF\n",
    "X_train_vif = pd.DataFrame(X_train, columns=X_train.columns)\n",
    "X_train_vif['Intercept'] = 1\n",
    "\n",
    "vif_data = pd.DataFrame({\n",
    "    'Feature': X_train_vif.columns,\n",
    "    'VIF': [variance_inflation_factor(X_train_vif.values, i) for i in range(X_train_vif.shape[1])]\n",
    "})\n",
    "\n",
    "# Remove intercept from VIF output\n",
    "vif_data = vif_data[vif_data['Feature'] != 'Intercept']\n",
    "\n",
    "# Display VIF DataFrame\n",
    "print(\"\\nVariance Inflation Factor (VIF):\")\n",
    "print(vif_data)\n",
    "\n",
    "# Optional: Plot VIF for visualization\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(vif_data['Feature'], vif_data['VIF'])\n",
    "plt.xlabel('VIF')\n",
    "plt.title('Variance Inflation Factor (Multicollinearity)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the saved model name and parameter grid\n",
    "saved_model_name = 'model_lr_TUNED.joblib'\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],\n",
    "}\n",
    "\n",
    "# Train or load the tuned logistic regression model\n",
    "if os.path.exists(saved_model_name):\n",
    "    loaded_model = joblib.load(saved_model_name)\n",
    "    model_lr_TUNED = loaded_model\n",
    "else:\n",
    "    grid_search = GridSearchCV(LogisticRegression(), param_grid, cv=5)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    model_lr_TUNED = grid_search.best_estimator_\n",
    "    joblib.dump(model_lr_TUNED, saved_model_name)\n",
    "\n",
    "# Model predictions and evaluation\n",
    "y_pred = model_lr_TUNED.predict(X_test)\n",
    "\n",
    "train_score = model_lr_TUNED.score(X_train, y_train)\n",
    "test_score = model_lr_TUNED.score(X_test, y_test)\n",
    "\n",
    "model_scores['Logistic Regression TUNED'] = {\n",
    "    'Train Score': train_score,\n",
    "    'Test Score': test_score\n",
    "}\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(f\"Train Score: {train_score}\")\n",
    "print(f\"Test Score: {test_score}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Feature importance from logistic regression coefficients\n",
    "coefficients = model_lr_TUNED.coef_[0]\n",
    "\n",
    "# Create a DataFrame to hold feature importance data\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Coefficient': coefficients,\n",
    "    'Importance': np.abs(coefficients)\n",
    "})\n",
    "\n",
    "# Sort the features by their importance (absolute value of coefficients)\n",
    "feature_importance = feature_importance.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Plot the feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance['Feature'], feature_importance['Importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Feature Importance for Tuned Logistic Regression Model')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Optionally print the feature importance table\n",
    "print(\"\\nFeature Importance Table:\")\n",
    "print(feature_importance)\n",
    "\n",
    "# Multicollinearity Analysis: Variance Inflation Factor (VIF)\n",
    "# Add a constant to the training set for VIF computation\n",
    "X_train_vif = pd.DataFrame(X_train, columns=X_train.columns)\n",
    "X_train_vif['Intercept'] = 1\n",
    "\n",
    "# Compute VIF for each feature\n",
    "vif_data = pd.DataFrame({\n",
    "    'Feature': X_train_vif.columns,\n",
    "    'VIF': [variance_inflation_factor(X_train_vif.values, i) for i in range(X_train_vif.shape[1])]\n",
    "})\n",
    "\n",
    "# Remove intercept from VIF results\n",
    "vif_data = vif_data[vif_data['Feature'] != 'Intercept']\n",
    "\n",
    "# Display VIF DataFrame\n",
    "print(\"\\nVariance Inflation Factor (VIF):\")\n",
    "print(vif_data)\n",
    "\n",
    "# Plot VIF for visualization\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(vif_data['Feature'], vif_data['VIF'])\n",
    "plt.xlabel('VIF')\n",
    "plt.title('Variance Inflation Factor (Multicollinearity)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Step 1: Calculate Variance Inflation Factor (VIF) to detect multicollinearity\n",
    "def calculate_vif(X):\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"Feature\"] = X.columns\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "    return vif_data\n",
    "\n",
    "# Step 2: Preprocessing and scaling\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Standardizing the features\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Step 3: Handle Multicollinearity\n",
    "print(\"Calculating VIF for initial features...\")\n",
    "vif_data = calculate_vif(pd.DataFrame(X_train_scaled, columns=X_train.columns))\n",
    "print(\"VIF data before removing collinearity:\")\n",
    "print(vif_data)\n",
    "\n",
    "# Dropping features with VIF > 5 (multicollinearity threshold)\n",
    "vif_threshold = 5\n",
    "high_vif_features = vif_data[vif_data[\"VIF\"] > vif_threshold][\"Feature\"]\n",
    "print(f\"Features to drop due to multicollinearity (VIF > {vif_threshold}): {list(high_vif_features)}\")\n",
    "\n",
    "# Remove high VIF features from both training and test sets\n",
    "X_train_reduced = X_train.drop(columns=high_vif_features)\n",
    "X_test_reduced = X_test.drop(columns=high_vif_features)\n",
    "\n",
    "# Step 4: Scaling the reduced feature set\n",
    "X_train_scaled_reduced = scaler.fit_transform(X_train_reduced)\n",
    "X_test_scaled_reduced = scaler.transform(X_test_reduced)\n",
    "\n",
    "# Step 5: Define and train the HistGradientBoostingClassifier model\n",
    "saved_model_name = 'model_HGBC_reduced.joblib'\n",
    "\n",
    "if os.path.exists(saved_model_name):\n",
    "    loaded_model = joblib.load(saved_model_name)\n",
    "    model_hgb = loaded_model\n",
    "else:\n",
    "    model_hgb = HistGradientBoostingClassifier()\n",
    "    model_hgb.fit(X_train_scaled_reduced, y_train)\n",
    "    joblib.dump(model_hgb, saved_model_name)\n",
    "\n",
    "# Step 6: Evaluate the model\n",
    "y_pred = model_hgb.predict(X_test_scaled_reduced)\n",
    "train_score = model_hgb.score(X_train_scaled_reduced, y_train)\n",
    "test_score = model_hgb.score(X_test_scaled_reduced, y_test)\n",
    "\n",
    "model_scores = {}\n",
    "model_scores['Hist Gradient Boosting (Reduced Features)'] = {\n",
    "    'Train Score': train_score,\n",
    "    'Test Score': test_score,\n",
    "}\n",
    "\n",
    "print(f\"Train Score: {train_score}\")\n",
    "print(f\"Test Score: {test_score}\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Step 7: Visualize feature importance\n",
    "# Extract feature importance from the model\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X_train_reduced.columns,\n",
    "    'Importance': model_hgb.feature_importances_\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance['Feature'], feature_importance['Importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Feature Importance for Hist Gradient Boosting Classifier (Reduced Features)')\n",
    "plt.show()\n",
    "\n",
    "print(\"Feature Importance:\")\n",
    "print(feature_importance)\n",
    "\n",
    "# Optional: You can display the VIF data before and after removing high VIF features\n",
    "print(\"VIF data after removing collinearity:\")\n",
    "vif_data_reduced = calculate_vif(pd.DataFrame(X_train_scaled_reduced, columns=X_train_reduced.columns))\n",
    "print(vif_data_reduced)\n",
    "\n",
    "# Step 8: Plot VIF graph\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(vif_data[\"Feature\"], vif_data[\"VIF\"], color='blue', alpha=0.7)\n",
    "plt.axvline(x=vif_threshold, color='red', linestyle='--', label=f'VIF Threshold: {vif_threshold}')\n",
    "plt.xlabel('VIF')\n",
    "plt.title('VIF of Features (Before Removing Multicollinearity)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot VIF graph after feature removal\n",
    "plt.figure(figsize=(10, 6))\n",
    "vif_data_reduced = calculate_vif(pd.DataFrame(X_train_scaled_reduced, columns=X_train_reduced.columns))\n",
    "plt.barh(vif_data_reduced[\"Feature\"], vif_data_reduced[\"VIF\"], color='green', alpha=0.7)\n",
    "plt.axvline(x=vif_threshold, color='red', linestyle='--', label=f'VIF Threshold: {vif_threshold}')\n",
    "plt.xlabel('VIF')\n",
    "plt.title('VIF of Features (After Removing Multicollinearity)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Step 1: Calculate Variance Inflation Factor (VIF) to detect multicollinearity\n",
    "def calculate_vif(X):\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"Feature\"] = X.columns\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "    return vif_data\n",
    "\n",
    "# Model saving/loading\n",
    "saved_model_name = 'model_HGBC_TUNED.joblib'\n",
    "\n",
    "if os.path.exists(saved_model_name):\n",
    "    model_HGBC_TUNED = joblib.load(saved_model_name)\n",
    "else:\n",
    "    model_HGBC_TUNED = HistGradientBoostingClassifier()\n",
    "    param_grid = {\n",
    "        'max_iter': [100, 200, 300],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'max_depth': [3, 5, 7, 9, 21, 25],\n",
    "        'min_samples_leaf': [1, 5, 10, 15, 20]\n",
    "    }\n",
    "\n",
    "    grid_search = GridSearchCV(estimator=model_HGBC_TUNED, param_grid=param_grid, cv=5, n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_model_HGBC_TUNED = grid_search.best_estimator_\n",
    "    joblib.dump(best_model_HGBC_TUNED, saved_model_name)\n",
    "    print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "    model_HGBC_TUNED = best_model_HGBC_TUNED\n",
    "\n",
    "# Predictions and evaluation\n",
    "y_pred = model_HGBC_TUNED.predict(X_test)\n",
    "\n",
    "train_score = model_HGBC_TUNED.score(X_train, y_train)\n",
    "test_score = model_HGBC_TUNED.score(X_test, y_test)\n",
    "\n",
    "model_scores = {}\n",
    "model_scores['Hist Gradient Boosting TUNED'] = {\n",
    "    'Train Score': train_score,\n",
    "    'Test Score': test_score,\n",
    "}\n",
    "\n",
    "print(f\"Train Score: {train_score}\")\n",
    "print(f\"Test Score: {test_score}\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Step 2: Calculate VIF before removing high collinearity features\n",
    "print(\"Calculating VIF for initial features...\")\n",
    "vif_data = calculate_vif(X_train)\n",
    "print(\"VIF data before removing collinearity:\")\n",
    "print(vif_data)\n",
    "\n",
    "# Optional: You can drop features with high VIF (greater than a threshold, e.g., VIF > 5)\n",
    "vif_threshold = 5\n",
    "high_vif_features = vif_data[vif_data[\"VIF\"] > vif_threshold][\"Feature\"]\n",
    "print(f\"Features to drop due to multicollinearity (VIF > {vif_threshold}): {list(high_vif_features)}\")\n",
    "\n",
    "# Remove high VIF features from both training and test sets\n",
    "X_train_reduced = X_train.drop(columns=high_vif_features)\n",
    "X_test_reduced = X_test.drop(columns=high_vif_features)\n",
    "\n",
    "# Step 3: Feature Importance Extraction (with reduced features if applicable)\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X_train_reduced.columns if high_vif_features.any() else X_train.columns,\n",
    "    'Importance': model_HGBC_TUNED.feature_importances_  # Feature importances from the tuned model\n",
    "})\n",
    "\n",
    "# Sort features by importance\n",
    "feature_importance = feature_importance.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Step 4: Plot Feature Importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance['Feature'], feature_importance['Importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Feature Importance (HistGradientBoostingClassifier TUNED)')\n",
    "plt.show()\n",
    "\n",
    "# Optional: VIF after feature reduction\n",
    "print(\"VIF data after removing collinearity:\")\n",
    "vif_data_reduced = calculate_vif(X_train_reduced)\n",
    "print(vif_data_reduced)\n",
    "\n",
    "# Step 5: Plot VIF Graph\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(vif_data[\"Feature\"], vif_data[\"VIF\"], color='blue', alpha=0.7)\n",
    "plt.axvline(x=vif_threshold, color='red', linestyle='--', label=f'VIF Threshold: {vif_threshold}')\n",
    "plt.xlabel('VIF')\n",
    "plt.title('VIF of Features (Before Removing Multicollinearity)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot VIF graph after feature removal\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(vif_data_reduced[\"Feature\"], vif_data_reduced[\"VIF\"], color='green', alpha=0.7)\n",
    "plt.axvline(x=vif_threshold, color='red', linestyle='--', label=f'VIF Threshold: {vif_threshold}')\n",
    "plt.xlabel('VIF')\n",
    "plt.title('VIF of Features (After Removing Multicollinearity)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "qQTZpvSCTVqG",
    "outputId": "15518315-b3e2-4eb4-d7ec-0ae85b2beddc"
   },
   "outputs": [],
   "source": [
    "saved_model_name = 'model_KNN.joblib'\n",
    "\n",
    "if os.path.exists(saved_model_name):\n",
    "    loaded_model = joblib.load(saved_model_name)\n",
    "    model_knn = loaded_model\n",
    "else:\n",
    "    model_knn = KNeighborsClassifier()\n",
    "    model_knn.fit(X_train, y_train)\n",
    "    joblib.dump(model_knn, saved_model_name)\n",
    "\n",
    "y_pred = model_knn.predict(X_test)\n",
    "\n",
    "train_score = model_knn.score(X_train, y_train)\n",
    "test_score = model_knn.score(X_test, y_test)\n",
    "\n",
    "params = model_knn.get_params()\n",
    "\n",
    "model_scores['KNN'] = {\n",
    "    'Train Score': train_score,\n",
    "    'Test Score': test_score,\n",
    "    'Used parameters' : params\n",
    "}\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(f\"Train Score: {train_score}\")\n",
    "print(f\"Test Score: {test_score}\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "saved_model_name = 'model_KNN_TUNED.joblib'\n",
    "\n",
    "param_grid = {\n",
    "    'n_neighbors': [3, 5, 7, 9, 11]\n",
    "}\n",
    "\n",
    "if os.path.exists(saved_model_name):\n",
    "    loaded_model = joblib.load(saved_model_name)\n",
    "    model_knn_TUNED = loaded_model\n",
    "else:\n",
    "    grid_search = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    model_knn_TUNED = grid_search.best_estimator_\n",
    "    print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "    joblib.dump(model_knn_TUNED, saved_model_name)\n",
    "\n",
    "y_pred = model_knn_TUNED.predict(X_test)\n",
    "\n",
    "train_score = model_knn_TUNED.score(X_train, y_train)\n",
    "test_score = model_knn_TUNED.score(X_test, y_test)\n",
    "\n",
    "params = model_knn_TUNED.get_params()\n",
    "\n",
    "model_scores['KNN TUNED'] = {\n",
    "    'Train Score': train_score,\n",
    "    'Test Score': test_score,\n",
    "    'Used parameters' : params\n",
    "}\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(f\"Train Score: {train_score}\")\n",
    "print(f\"Test Score: {test_score}\")\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_KNN_D = KNN_D()\n",
    "model_KNN_D.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_train_pred = model_KNN_D.predict(X_train)\n",
    "y_test_pred = model_KNN_D.predict(X_test)\n",
    "\n",
    "\n",
    "train_score = accuracy_score(y_train, y_train_pred)\n",
    "test_score = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "model_scores['KNN_D'] = {\n",
    "    'Train Score': train_score,\n",
    "    'Test Score': test_score,\n",
    "    'Used parameters' : 'dcalc = Euclidean , NN = 5'\n",
    "}\n",
    "\n",
    "print(f\"Train Score: {train_score}\")\n",
    "print(f\"Test Score: {test_score}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_name = 'model_XGBoost.joblib'\n",
    "\n",
    "\n",
    "if os.path.exists(saved_model_name):\n",
    "    loaded_model = joblib.load(saved_model_name)\n",
    "    model_xgboost = loaded_model\n",
    "else:\n",
    "    model_xgboost = XGBClassifier()\n",
    "    model_xgboost.fit(X_train, y_train)\n",
    "    print(f\"Model Parameters: {model_xgboost.get_params()}\")\n",
    "    joblib.dump(model_xgboost, saved_model_name)\n",
    "\n",
    "y_pred = model_xgboost.predict(X_test)\n",
    "\n",
    "train_score = model_xgboost.score(X_train, y_train)\n",
    "test_score = model_xgboost.score(X_test, y_test)\n",
    "\n",
    "params = model_xgboost.get_params()\n",
    "\n",
    "model_scores['XGBoost Classifier'] = {\n",
    "    'Train Score': train_score,\n",
    "    'Test Score': test_score,\n",
    "    'Used parameters' : params\n",
    "}\n",
    "\n",
    "print(f\"Train Score: {train_score}\")\n",
    "print(f\"Test Score: {test_score}\")\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_name = 'model_XGBoost_TUNED.joblib'\n",
    "\n",
    "if os.path.exists(saved_model_name):\n",
    "    loaded_model = joblib.load(saved_model_name)\n",
    "    model_xgboost_TUNED = loaded_model\n",
    "else:\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'subsample': [0.8, 1.0],\n",
    "        'colsample_bytree': [0.8, 1.0]\n",
    "    }\n",
    "    grid_search = GridSearchCV(XGBClassifier(), param_grid, cv=5, n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    model_xgboost_TUNED = grid_search.best_estimator_\n",
    "    joblib.dump(model_xgboost_TUNED, saved_model_name)\n",
    "\n",
    "y_pred = model_xgboost_TUNED.predict(X_test)\n",
    "\n",
    "train_score = model_xgboost_TUNED.score(X_train, y_train)\n",
    "test_score = model_xgboost_TUNED.score(X_test, y_test)\n",
    "\n",
    "params = model_xgboost_TUNED.get_params()\n",
    "\n",
    "model_scores['XGBoost Classifier TUNED'] = {\n",
    "    'Train Score': train_score,\n",
    "    'Test Score': test_score,\n",
    "    'Used parameters' : params\n",
    "}\n",
    "\n",
    "print(f\"Train Score: {train_score}\")\n",
    "print(f\"Test Score: {test_score}\")\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Scaling the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)  # Fit and transform training data\n",
    "X_test_scaled = scaler.transform(X_test)        # Only transform test data\n",
    "\n",
    "# Define saved model name\n",
    "saved_model_name = 'model_ridge.joblib'\n",
    "\n",
    "# Train or load the RidgeClassifier\n",
    "if os.path.exists(saved_model_name):\n",
    "    loaded_model = joblib.load(saved_model_name)\n",
    "    model_ridge = loaded_model\n",
    "else:\n",
    "    model_ridge = RidgeClassifier()\n",
    "    model_ridge.fit(X_train_scaled, y_train)\n",
    "    joblib.dump(model_ridge, saved_model_name)\n",
    "\n",
    "# Predictions and evaluation\n",
    "y_pred = model_ridge.predict(X_test_scaled)\n",
    "\n",
    "train_score = model_ridge.score(X_train_scaled, y_train)\n",
    "test_score = model_ridge.score(X_test_scaled, y_test)\n",
    "\n",
    "model_scores['Ridge Classifier'] = {\n",
    "    'Train Score': train_score,\n",
    "    'Test Score': test_score\n",
    "}\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(f\"Train Score: {train_score}\")\n",
    "print(f\"Test Score: {test_score}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Feature importance from RidgeClassifier coefficients\n",
    "coefficients = model_ridge.coef_[0]\n",
    "\n",
    "# Create a DataFrame to hold feature importance data\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Coefficient': coefficients,\n",
    "    'Importance': np.abs(coefficients)\n",
    "})\n",
    "\n",
    "# Sort features by importance (absolute value of coefficients)\n",
    "feature_importance = feature_importance.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance['Feature'], feature_importance['Importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Feature Importance for Ridge Classifier')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Optionally print the feature importance table\n",
    "print(\"\\nFeature Importance Table:\")\n",
    "print(feature_importance)\n",
    "\n",
    "# Multicollinearity Analysis: Variance Inflation Factor (VIF)\n",
    "# Add a constant for VIF computation\n",
    "X_train_vif = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "X_train_vif['Intercept'] = 1\n",
    "\n",
    "# Compute VIF for each feature\n",
    "vif_data = pd.DataFrame({\n",
    "    'Feature': X_train_vif.columns,\n",
    "    'VIF': [variance_inflation_factor(X_train_vif.values, i) for i in range(X_train_vif.shape[1])]\n",
    "})\n",
    "\n",
    "# Remove the intercept from VIF results\n",
    "vif_data = vif_data[vif_data['Feature'] != 'Intercept']\n",
    "\n",
    "# Display VIF DataFrame\n",
    "print(\"\\nVariance Inflation Factor (VIF):\")\n",
    "print(vif_data)\n",
    "\n",
    "# Plot VIF for visualization\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(vif_data['Feature'], vif_data['VIF'])\n",
    "plt.xlabel('VIF')\n",
    "plt.title('Variance Inflation Factor (Multicollinearity)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Scaling the data before fitting the model\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Only transform the test data\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Save/load tuned RidgeClassifier\n",
    "saved_model_name = 'model_ridge_TUNED.joblib'\n",
    "\n",
    "if os.path.exists(saved_model_name):\n",
    "    model_ridge_TUNED = joblib.load(saved_model_name)\n",
    "else:\n",
    "    param_grid = {\n",
    "        'alpha': [0.1, 1.0, 10.0, 100.0],\n",
    "        'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga', 'lbfgs']\n",
    "    }\n",
    "    ridge = RidgeClassifier()\n",
    "    grid_search = GridSearchCV(ridge, param_grid, cv=5, scoring='accuracy')\n",
    "    grid_search.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    model_ridge_TUNED = grid_search.best_estimator_\n",
    "    joblib.dump(model_ridge_TUNED, saved_model_name)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model_ridge_TUNED.predict(X_test_scaled)\n",
    "\n",
    "train_score = model_ridge_TUNED.score(X_train_scaled, y_train)\n",
    "test_score = model_ridge_TUNED.score(X_test_scaled, y_test)\n",
    "\n",
    "model_scores['Ridge Classifier (Tuned)'] = {\n",
    "    'Train Score': train_score,\n",
    "    'Test Score': test_score\n",
    "}\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(f\"Train Score: {train_score}\")\n",
    "print(f\"Test Score: {test_score}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Feature importance analysis\n",
    "coefficients = model_ridge_TUNED.coef_[0]\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Coefficient': coefficients,\n",
    "    'Importance': np.abs(coefficients)\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance['Feature'], feature_importance['Importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Feature Importance for Ridge Classifier (Tuned)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nFeature Importance Table:\")\n",
    "print(feature_importance)\n",
    "\n",
    "# Multicollinearity Analysis: Variance Inflation Factor (VIF)\n",
    "# Prepare data for VIF computation\n",
    "X_train_vif = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "X_train_vif['Intercept'] = 1\n",
    "\n",
    "# Compute VIF for each feature\n",
    "vif_data = pd.DataFrame({\n",
    "    'Feature': X_train_vif.columns,\n",
    "    'VIF': [variance_inflation_factor(X_train_vif.values, i) for i in range(X_train_vif.shape[1])]\n",
    "})\n",
    "\n",
    "# Remove intercept from VIF results\n",
    "vif_data = vif_data[vif_data['Feature'] != 'Intercept']\n",
    "\n",
    "# Display and plot VIF\n",
    "print(\"\\nVariance Inflation Factor (VIF):\")\n",
    "print(vif_data)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(vif_data['Feature'], vif_data['VIF'])\n",
    "plt.xlabel('VIF')\n",
    "plt.title('Variance Inflation Factor (Multicollinearity)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Optional: Remove features with high VIF and refit the model\n",
    "high_vif_features = vif_data[vif_data['VIF'] > 10]['Feature']\n",
    "if not high_vif_features.empty:\n",
    "    print(\"\\nFeatures with high multicollinearity (VIF > 10):\")\n",
    "    print(high_vif_features.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import joblib\n",
    "import os\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# PREPROCESSING\n",
    "# Standardizing the features\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# MULTICOLLINEARITY DETECTION\n",
    "# Calculate VIF for each feature\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"Feature\"] = X_train.columns  # Assuming X_train is a DataFrame\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X_train_scaled, i) for i in range(X_train_scaled.shape[1])]\n",
    "\n",
    "# Print VIF values\n",
    "print(\"Variance Inflation Factor (VIF) for features:\")\n",
    "print(vif_data)\n",
    "\n",
    "# Optionally drop features with high VIF (threshold depends on your tolerance, typically > 10 indicates severe multicollinearity)\n",
    "high_vif_features = vif_data[vif_data[\"VIF\"] > 10][\"Feature\"].tolist()\n",
    "print(f\"Features with high VIF: {high_vif_features}\")\n",
    "\n",
    "# Drop high VIF features (optional)\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns).drop(columns=high_vif_features)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_train.columns).drop(columns=high_vif_features)\n",
    "\n",
    "# TRAINING\n",
    "saved_model_name = 'model_lasso_classifier.joblib'\n",
    "\n",
    "# Load the model if it exists, otherwise fit a new one\n",
    "if os.path.exists(saved_model_name):\n",
    "    loaded_model = joblib.load(saved_model_name)\n",
    "    model_lasso_classifier = loaded_model\n",
    "else:\n",
    "    model_lasso_classifier = LogisticRegression(penalty='l1', solver='saga', max_iter=1000)\n",
    "    model_lasso_classifier.fit(X_train_scaled, y_train)\n",
    "    joblib.dump(model_lasso_classifier, saved_model_name)\n",
    "\n",
    "# EVALUATION\n",
    "y_pred = model_lasso_classifier.predict(X_test_scaled)\n",
    "\n",
    "train_score = model_lasso_classifier.score(X_train_scaled, y_train)\n",
    "test_score = model_lasso_classifier.score(X_test_scaled, y_test)\n",
    "\n",
    "# Store model scores\n",
    "model_scores['Lasso Classifier'] = {\n",
    "    'Train Score': train_score,\n",
    "    'Test Score': test_score\n",
    "}\n",
    "\n",
    "# Confusion Matrix and Classification Report\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(f\"Train Score: {train_score}\")\n",
    "print(f\"Test Score: {test_score}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# FEATURE IMPORTANCE\n",
    "# Extracting coefficients (importance)\n",
    "coefficients = model_lasso_classifier.coef_[0]  # Coefficients for binary classification (only one array)\n",
    "\n",
    "# Creating a DataFrame to show features and their corresponding importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X_train.columns,  # Updated X_train after removing high VIF features\n",
    "    'Coefficient': coefficients,\n",
    "    'Importance': np.abs(coefficients)  # Using absolute value of coefficients for importance\n",
    "})\n",
    "\n",
    "# Sorting by importance (largest to smallest)\n",
    "feature_importance = feature_importance.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Plotting feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance['Feature'], feature_importance['Importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Feature Importance for Lasso Classifier')\n",
    "plt.show()\n",
    "\n",
    "# Optionally, display the feature importance table\n",
    "print(feature_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Step 1: Calculate Variance Inflation Factor (VIF) to detect multicollinearity\n",
    "def calculate_vif(X):\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"Feature\"] = X.columns\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "    return vif_data\n",
    "\n",
    "# Step 2: Preprocessing and scaling\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Standardizing the features\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Step 3: Handle Multicollinearity\n",
    "print(\"Calculating VIF for initial features...\")\n",
    "vif_data = calculate_vif(pd.DataFrame(X_train_scaled, columns=X_train.columns))\n",
    "print(\"VIF data before removing collinearity:\")\n",
    "print(vif_data)\n",
    "\n",
    "# Dropping features with VIF > 5 (multicollinearity threshold)\n",
    "vif_threshold = 5\n",
    "high_vif_features = vif_data[vif_data[\"VIF\"] > vif_threshold][\"Feature\"]\n",
    "print(f\"Features to drop due to multicollinearity (VIF > {vif_threshold}): {list(high_vif_features)}\")\n",
    "\n",
    "# Remove high VIF features from both training and test sets\n",
    "X_train_reduced = X_train.drop(columns=high_vif_features)\n",
    "X_test_reduced = X_test.drop(columns=high_vif_features)\n",
    "\n",
    "# Step 4: Scaling the reduced feature set\n",
    "X_train_scaled_reduced = scaler.fit_transform(X_train_reduced)\n",
    "X_test_scaled_reduced = scaler.transform(X_test_reduced)\n",
    "\n",
    "# Step 5: Define and train the Lasso Logistic Regression model\n",
    "saved_model_name = 'model_lasso_classifier_TUNED_reduced.joblib'\n",
    "\n",
    "if os.path.exists(saved_model_name):\n",
    "    model_lasso_classifier = joblib.load(saved_model_name)\n",
    "else:\n",
    "    param_grid = {\n",
    "        'C': [0.01, 0.1, 1, 10, 100],\n",
    "        'max_iter': [1000, 2000]\n",
    "    }\n",
    "    base_model = LogisticRegression(penalty='l1', solver='saga')\n",
    "    grid_search = GridSearchCV(base_model, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "    grid_search.fit(X_train_scaled_reduced, y_train)\n",
    "    model_lasso_classifier = grid_search.best_estimator_\n",
    "    joblib.dump(model_lasso_classifier, saved_model_name)\n",
    "\n",
    "# Step 6: Evaluate the model\n",
    "y_pred = model_lasso_classifier.predict(X_test_scaled_reduced)\n",
    "train_score = model_lasso_classifier.score(X_train_scaled_reduced, y_train)\n",
    "test_score = model_lasso_classifier.score(X_test_scaled_reduced, y_test)\n",
    "\n",
    "model_scores = {}\n",
    "model_scores['Lasso Classifier (Tuned, Reduced Features)'] = {\n",
    "    'Train Score': train_score,\n",
    "    'Test Score': test_score\n",
    "}\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(f\"Train Score: {train_score}\")\n",
    "print(f\"Test Score: {test_score}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Step 7: Visualize feature importance\n",
    "coefficients = model_lasso_classifier.coef_[0]\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X_train_reduced.columns,\n",
    "    'Coefficient': coefficients,\n",
    "    'Importance': np.abs(coefficients)\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance['Feature'], feature_importance['Importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Feature Importance for Tuned Lasso Classifier (Reduced Features)')\n",
    "plt.show()\n",
    "\n",
    "print(\"Feature Importance:\")\n",
    "print(feature_importance)\n",
    "\n",
    "# Optional: You can display the VIF data before and after removing high VIF features\n",
    "print(\"VIF data after removing collinearity:\")\n",
    "vif_data_reduced = calculate_vif(pd.DataFrame(X_train_scaled_reduced, columns=X_train_reduced.columns))\n",
    "print(vif_data_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_name = 'model_DTC.joblib'\n",
    "\n",
    "if os.path.exists(saved_model_name):\n",
    "    loaded_model = joblib.load(saved_model_name)\n",
    "    model_DTC = loaded_model\n",
    "else:\n",
    "    model_DTC = DecisionTreeClassifier(random_state=42)\n",
    "    model_DTC.fit(X_train, y_train)\n",
    "    joblib.dump(model_DTC, saved_model_name)\n",
    "\n",
    "y_pred = model_DTC.predict(X_test)\n",
    "\n",
    "train_score = model_DTC.score(X_train, y_train)\n",
    "test_score = model_DTC.score(X_test, y_test)\n",
    "\n",
    "params = model_DTC.get_params()\n",
    "\n",
    "model_scores['Decision Tree Classifier'] = {\n",
    "    'Train Score': train_score,\n",
    "    'Test Score': test_score,\n",
    "    'Used parameters' : params\n",
    "}\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(f\"Train Score: {train_score}\")\n",
    "print(f\"Test Score: {test_score}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_name = 'model_DTC_TUNED.joblib'\n",
    "\n",
    "if os.path.exists(saved_model_name):\n",
    "    loaded_model = joblib.load(saved_model_name)\n",
    "    model_DT_TUNED = loaded_model\n",
    "else:\n",
    "    param_grid = {\n",
    "        'criterion': ['gini', 'entropy'],\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    }\n",
    "    model_DT_TUNED = DecisionTreeClassifier(random_state=42)\n",
    "    grid_search = GridSearchCV(estimator=model_DT_TUNED, param_grid=param_grid, cv=5, n_jobs=-1, scoring='accuracy')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    model_DT_TUNED = grid_search.best_estimator_\n",
    "    joblib.dump(model_DT_TUNED, saved_model_name)\n",
    "\n",
    "y_pred = model_DT_TUNED.predict(X_test)\n",
    "\n",
    "train_score = model_DT_TUNED.score(X_train, y_train)\n",
    "test_score = model_DT_TUNED.score(X_test, y_test)\n",
    "\n",
    "params = model_DT_TUNED.get_params()\n",
    "\n",
    "model_scores['Decision Tree Classifier TUNED'] = {\n",
    "    'Train Score': train_score,\n",
    "    'Test Score': test_score,\n",
    "    'Used parameters' : params\n",
    "}\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(f\"Train Score: {train_score}\")\n",
    "print(f\"Test Score: {test_score}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "TeiW2Ei4leBm",
    "outputId": "2fd30065-1ae0-4934-cef5-f81aad4454d2"
   },
   "outputs": [],
   "source": [
    "saved_model_name = 'model_RandomForestClassifier.joblib'\n",
    "\n",
    "if os.path.exists(saved_model_name):\n",
    "    loaded_model = joblib.load(saved_model_name)\n",
    "    model_RF = loaded_model\n",
    "else:\n",
    "    model_RF = RandomForestClassifier(random_state=42)\n",
    "    model_RF.fit(X_train, y_train)\n",
    "    joblib.dump(model_RF, saved_model_name)\n",
    "\n",
    "y_pred = model_RF.predict(X_test)\n",
    "\n",
    "train_score = model_RF.score(X_train, y_train)\n",
    "test_score = model_RF.score(X_test, y_test)\n",
    "\n",
    "params = model_RF.get_params()\n",
    "\n",
    "model_scores['Random Forest Classifier'] = {\n",
    "    'Train Score': train_score,\n",
    "    'Test Score': test_score,\n",
    "    'Used parameters' : params\n",
    "}\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(f\"Train Score: {train_score}\")\n",
    "print(f\"Test Score: {test_score}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = model_RF.feature_importances_\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': importances\n",
    "})\n",
    "\n",
    "feature_importance = feature_importance.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance['Feature'], feature_importance['Importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Feature Importance (Random Forest)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_name = 'model_RandomForest_TUNED.joblib'\n",
    "\n",
    "if os.path.exists(saved_model_name):\n",
    "    loaded_model = joblib.load(saved_model_name)\n",
    "    model_RF_TUNED = loaded_model\n",
    "else:\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [None, 10, 20],\n",
    "        'min_samples_split': [2, 5],\n",
    "        'min_samples_leaf': [1, 2]\n",
    "    }\n",
    "    grid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5, n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    model_RF_TUNED = grid_search.best_estimator_\n",
    "    joblib.dump(model_RF_TUNED, saved_model_name)\n",
    "\n",
    "y_pred = model_RF_TUNED.predict(X_test)\n",
    "\n",
    "train_score = model_RF_TUNED.score(X_train, y_train)\n",
    "test_score = model_RF_TUNED.score(X_test, y_test)\n",
    "\n",
    "params = model_RF_TUNED.get_params()\n",
    "\n",
    "model_scores['Random Forest Classifier TUNED'] = {\n",
    "    'Train Score': train_score,\n",
    "    'Test Score': test_score,\n",
    "    'Used parameters' : params\n",
    "}\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(f\"Train Score: {train_score}\")\n",
    "print(f\"Test Score: {test_score}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = model_RF_TUNED.feature_importances_\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': importances\n",
    "})\n",
    "\n",
    "feature_importance = feature_importance.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance['Feature'], feature_importance['Importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Feature Importance (Random Forest)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Step 1: Calculate Variance Inflation Factor (VIF) to detect multicollinearity\n",
    "def calculate_vif(X):\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"Feature\"] = X.columns\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "    return vif_data\n",
    "\n",
    "# Step 2: Preprocessing and scaling\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Standardizing the features\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Step 3: Handle Multicollinearity\n",
    "print(\"Calculating VIF for initial features...\")\n",
    "vif_data = calculate_vif(pd.DataFrame(X_train_scaled, columns=X_train.columns))\n",
    "print(\"VIF data before removing collinearity:\")\n",
    "print(vif_data)\n",
    "\n",
    "# Dropping features with VIF > 5 (multicollinearity threshold)\n",
    "vif_threshold = 5\n",
    "high_vif_features = vif_data[vif_data[\"VIF\"] > vif_threshold][\"Feature\"]\n",
    "print(f\"Features to drop due to multicollinearity (VIF > {vif_threshold}): {list(high_vif_features)}\")\n",
    "\n",
    "# Remove high VIF features from both training and test sets\n",
    "X_train_reduced = X_train.drop(columns=high_vif_features)\n",
    "X_test_reduced = X_test.drop(columns=high_vif_features)\n",
    "\n",
    "# Step 4: Scaling the reduced feature set\n",
    "X_train_scaled_reduced = scaler.fit_transform(X_train_reduced)\n",
    "X_test_scaled_reduced = scaler.transform(X_test_reduced)\n",
    "\n",
    "# Step 5: Define and train the SVM model\n",
    "saved_model_name = 'model_svm_classifier_DEFAULT_reduced.joblib'\n",
    "\n",
    "if os.path.exists(saved_model_name):\n",
    "    model_svm_classifier = joblib.load(saved_model_name)\n",
    "else:\n",
    "    model_svm_classifier = SVC()  # Default parameters\n",
    "    model_svm_classifier.fit(X_train_scaled_reduced, y_train)\n",
    "    joblib.dump(model_svm_classifier, saved_model_name)\n",
    "\n",
    "# Step 6: Evaluate the model\n",
    "y_pred = model_svm_classifier.predict(X_test_scaled_reduced)\n",
    "train_score = model_svm_classifier.score(X_train_scaled_reduced, y_train)\n",
    "test_score = model_svm_classifier.score(X_test_scaled_reduced, y_test)\n",
    "\n",
    "model_scores = {}\n",
    "model_scores['SVM Classifier (Reduced Features)'] = {\n",
    "    'Train Score': train_score,\n",
    "    'Test Score': test_score\n",
    "}\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Print results\n",
    "print(f\"Train Score: {train_score}\")\n",
    "print(f\"Test Score: {test_score}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Step 7: Visualize feature importance (using coefficients)\n",
    "# In SVM, coefficients represent feature importance\n",
    "coefficients = model_svm_classifier.coef_[0]\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X_train_reduced.columns,\n",
    "    'Coefficient': coefficients,\n",
    "    'Importance': np.abs(coefficients)\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance['Feature'], feature_importance['Importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Feature Importance for SVM Classifier (Reduced Features)')\n",
    "plt.show()\n",
    "\n",
    "print(\"Feature Importance:\")\n",
    "print(feature_importance)\n",
    "\n",
    "# Optional: You can display the VIF data before and after removing high VIF features\n",
    "print(\"VIF data after removing collinearity:\")\n",
    "vif_data_reduced = calculate_vif(pd.DataFrame(X_train_scaled_reduced, columns=X_train_reduced.columns))\n",
    "print(vif_data_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Step 1: Calculate Variance Inflation Factor (VIF) to detect multicollinearity\n",
    "def calculate_vif(X):\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"Feature\"] = X.columns\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "    return vif_data\n",
    "\n",
    "# Step 2: Preprocessing and scaling\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Standardizing the features\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Step 3: Handle Multicollinearity\n",
    "print(\"Calculating VIF for initial features...\")\n",
    "vif_data = calculate_vif(pd.DataFrame(X_train_scaled, columns=X_train.columns))\n",
    "print(\"VIF data before removing collinearity:\")\n",
    "print(vif_data)\n",
    "\n",
    "# Dropping features with VIF > 5 (multicollinearity threshold)\n",
    "vif_threshold = 5\n",
    "high_vif_features = vif_data[vif_data[\"VIF\"] > vif_threshold][\"Feature\"]\n",
    "print(f\"Features to drop due to multicollinearity (VIF > {vif_threshold}): {list(high_vif_features)}\")\n",
    "\n",
    "# Remove high VIF features from both training and test sets\n",
    "X_train_reduced = X_train.drop(columns=high_vif_features)\n",
    "X_test_reduced = X_test.drop(columns=high_vif_features)\n",
    "\n",
    "# Step 4: Scaling the reduced feature set\n",
    "X_train_scaled_reduced = scaler.fit_transform(X_train_reduced)\n",
    "X_test_scaled_reduced = scaler.transform(X_test_reduced)\n",
    "\n",
    "# Step 5: Define and train the SVM model\n",
    "saved_model_name = 'model_svm_classifier_DEFAULT_reduced.joblib'\n",
    "\n",
    "if os.path.exists(saved_model_name):\n",
    "    model_svm_classifier = joblib.load(saved_model_name)\n",
    "else:\n",
    "    model_svm_classifier = SVC()  # Default parameters\n",
    "    model_svm_classifier.fit(X_train_scaled_reduced, y_train)\n",
    "    joblib.dump(model_svm_classifier, saved_model_name)\n",
    "\n",
    "# Step 6: Evaluate the model\n",
    "y_pred = model_svm_classifier.predict(X_test_scaled_reduced)\n",
    "train_score = model_svm_classifier.score(X_train_scaled_reduced, y_train)\n",
    "test_score = model_svm_classifier.score(X_test_scaled_reduced, y_test)\n",
    "\n",
    "model_scores = {}\n",
    "model_scores['SVM Classifier (Reduced Features)'] = {\n",
    "    'Train Score': train_score,\n",
    "    'Test Score': test_score\n",
    "}\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Print results\n",
    "print(f\"Train Score: {train_score}\")\n",
    "print(f\"Test Score: {test_score}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Step 7: Visualize feature importance (using coefficients)\n",
    "# In SVM, coefficients represent feature importance\n",
    "coefficients = model_svm_classifier.coef_[0]\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X_train_reduced.columns,\n",
    "    'Coefficient': coefficients,\n",
    "    'Importance': np.abs(coefficients)\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance['Feature'], feature_importance['Importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Feature Importance for SVM Classifier (Reduced Features)')\n",
    "plt.show()\n",
    "\n",
    "print(\"Feature Importance:\")\n",
    "print(feature_importance)\n",
    "\n",
    "# Optional: You can display the VIF data before and after removing high VIF features\n",
    "print(\"VIF data after removing collinearity:\")\n",
    "vif_data_reduced = calculate_vif(pd.DataFrame(X_train_scaled_reduced, columns=X_train_reduced.columns))\n",
    "print(vif_data_reduced)\n",
    "\n",
    "# Step 8: Plot VIF graph\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(vif_data[\"Feature\"], vif_data[\"VIF\"], color='blue', alpha=0.7)\n",
    "plt.axvline(x=vif_threshold, color='red', linestyle='--', label=f'VIF Threshold: {vif_threshold}')\n",
    "plt.xlabel('VIF')\n",
    "plt.title('VIF of Features (Before Removing Multicollinearity)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot VIF graph after feature removal\n",
    "plt.figure(figsize=(10, 6))\n",
    "vif_data_reduced = calculate_vif(pd.DataFrame(X_train_scaled_reduced, columns=X_train_reduced.columns))\n",
    "plt.barh(vif_data_reduced[\"Feature\"], vif_data_reduced[\"VIF\"], color='green', alpha=0.7)\n",
    "plt.axvline(x=vif_threshold, color='red', linestyle='--', label=f'VIF Threshold: {vif_threshold}')\n",
    "plt.xlabel('VIF')\n",
    "plt.title('VIF of Features (After Removing Multicollinearity)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 582
    },
    "id": "yM-ecdgCQlhv",
    "outputId": "ccc13dce-8b41-45f2-bad2-9aa88f738d8c"
   },
   "outputs": [],
   "source": [
    "models = list(model_scores.keys())\n",
    "train_scores = [model_scores[model][\"Train Score\"] for model in models]\n",
    "test_scores = [model_scores[model][\"Test Score\"] for model in models]\n",
    "\n",
    "x = range(len(models))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(x, train_scores, width=0.4, label='Train Score', color='b', align='center')\n",
    "plt.bar([p + 0.4 for p in x], test_scores, width=0.4, label='Test Score', color='orange', align='center')\n",
    "\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('Scores')\n",
    "plt.title('Train and Test Scores of Different Models')\n",
    "plt.xticks([p + 0.2 for p in x], models , rotation =90)\n",
    "plt.ylim(0.9, 1)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in model_scores:\n",
    "        print(i)\n",
    "        print(f'Used parameters: {model_scores[i][\"Used parameters\"]}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find at employees at risk of leaving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_with_dummies.drop('STATUS_Beëindigd', axis=1)\n",
    "y = df_with_dummies['STATUS_Beëindigd']\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "kf = KFold(n_splits=10)\n",
    "\n",
    "predicted_vals = []\n",
    "\n",
    "for i, j in kf.split(X):\n",
    "    X_train, X_test = X.iloc[i], X.iloc[j]\n",
    "    y_train, y_test = y.iloc[i], y.iloc[j]\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    predicted_vals.extend(predictions)\n",
    "\n",
    "df_n_dummies = pd.get_dummies(df , columns=['STATUS'] , drop_first=True)\n",
    "df['pred_STATUS_Beëindigd'] = predicted_vals\n",
    "\n",
    "at_risk_employees = df_n_dummies[(df_n_dummies['pred_STATUS_Beëindigd'] == True) & (df_n_dummies['STATUS_Beëindigd'] == False)]\n",
    "at_risk_employees\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
